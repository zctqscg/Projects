{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176c93bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcb7a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location = \"/FileStore/tables/fraudTrain.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "train = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "display(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66231c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=train.union(test)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d5e3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show database\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4779f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show database with sql\n",
    "spark.sql(\"show databases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87335d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.catalog.listTables(db_name)\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d74ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"%sql\n",
    "DROP TABLE fraudTest_csv;\n",
    "DROP TABLE fraudTrain_csv;\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a6daf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('show tables from default').show() #global_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6849e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.dtypes:\n",
    "\n",
    "    print(col[0]+\" , \"+col[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0962ef0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in df.schema.fields:\n",
    "    print(field.name +\" , \"+str(field.dataType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866159b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d592e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "# get string\n",
    "str_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, StringType)]\n",
    "# ['colc']\n",
    "\n",
    "# or double\n",
    "num_cols = [f.name for f in df.schema.fields if isinstance(f.dataType, (DoubleType,IntegerType))]\n",
    "# ['colb']\n",
    "\n",
    "dbl= [f.name for f in df.schema.fields if isinstance(f.dataType, DoubleType)]\n",
    "# ['colb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2624fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnull, when, count, col\n",
    "nacounts = df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).toPandas()\n",
    "nacounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54186d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic solution \n",
    "amount_missing_df = train[num_cols].select([(count(when(isnan(c) | col(c).isNull(), c))/count(lit(1))).alias(c) for c in train[num_cols].columns])\n",
    "amount_missing_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d96b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud=df.filter(df.is_fraud==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de112108",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud.show(n=1, truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7275a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((df.count(), len(df.columns)))\n",
    "print((fraud.count(), len(fraud.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8358db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('category').count().orderBy('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3512b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_fraud=df.groupby(\"is_fraud\")\\\n",
    "   .pivot(\"category\")\\\n",
    "   .count()\\\n",
    "   .display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c850b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('is_fraud').agg(avg('amt').alias('average amount')).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee948a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('is_fraud') .agg(avg(\"amt\").alias(\"avg\"), \\\n",
    "         max(\"amt\").alias(\"max\") \\\n",
    "                              ).show()                       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd7423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['is_fraud','state']).count().orderBy(['is_fraud','count'], ascending=[False,False]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56cf299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df.select(F.countDistinct(\"_c0\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40266341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "df.groupBy(['is_fraud','state']).count()\\\n",
    ".withColumn('total', F.sum('count').over(Window.partitionBy()))\\\n",
    ".withColumn('percentage',F.col('count')/F.col('total'))\\\n",
    ".orderBy(['is_fraud','percentage'], ascending=[False,False])\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3113d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['is_fraud','state']).agg(\n",
    "    (F.count('_c0')).alias('count'),\n",
    "    (F.count('_c0') / df.count()).alias('percentage')\n",
    ").orderBy(['is_fraud','percentage'], ascending=[False,False]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc30c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ny=df.filter(df.state=='NY')\n",
    "ny.summary().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bb94f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = df[['first','last']].distinct() \n",
    "names.display(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad07d7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "names.drop_duplicates().count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
